{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_uri = 's3://sagemaker-eu-west-1-934765130326/sagemaker/Churn-xgboost/data/small/churn-dataset.csv'\n",
    "!aws s3 cp $s3_uri .\n",
    "\n",
    "data = pd.read_csv(\"./churn-dataset.csv\")\n",
    "pd.set_option(\"display.max_columns\", 500)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency tables for each categorical feature\n",
    "for column in data.select_dtypes(include=[\"object\"]).columns:\n",
    "    display(pd.crosstab(index=data[column], columns=\"% observations\", normalize=\"columns\"))\n",
    "\n",
    "# Histograms for each numeric features\n",
    "display(data.describe())\n",
    "%matplotlib inline\n",
    "hist = data.hist(bins=30, sharey=True, figsize=(10, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(\"Phone\", axis=1)\n",
    "data[\"Area Code\"] = data[\"Area Code\"].astype(object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize relationship between features and target variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in data.select_dtypes(include=[\"object\"]).columns:\n",
    "    if column != \"Churn?\":\n",
    "        display(pd.crosstab(index=data[column], columns=data[\"Churn?\"], normalize=\"columns\"))\n",
    "\n",
    "for column in data.select_dtypes(exclude=[\"object\"]).columns:\n",
    "    print(column)\n",
    "    hist = data[[column, \"Churn?\"]].hist(by=\"Churn?\", bins=30)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(data.corr())\n",
    "pd.plotting.scatter_matrix(data, figsize=(12, 12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove some features, because data science..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop([\"Day Charge\", \"Eve Charge\", \"Night Charge\", \"Intl Charge\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encode catagorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = pd.get_dummies(data)\n",
    "model_data = pd.concat(\n",
    "    [model_data[\"Churn?_True.\"], model_data.drop([\"Churn?_False.\", \"Churn?_True.\"], axis=1)], axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, validation_data, test_data = np.split(\n",
    "    model_data.sample(frac=1, random_state=1729),\n",
    "    [int(0.7 * len(model_data)), int(0.9 * len(model_data))],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer preprocessing code into scripts\n",
    "After some local, initial experiments, we want to transfer the code related to preprocessing the data and training the model into scripts that we can can check in to trigger the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../algortihms/1-preprocessing/preprocess.py\n",
    "\n",
    "\"\"\"Feature engineers the customer churn dataset.\"\"\"\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logger.info(\"Starting preprocessing.\")\n",
    "\n",
    "    _dir = \"/opt/ml/processing/input\"\n",
    "\n",
    "    input_data_path = \"\"\n",
    "\n",
    "    # We don't know the ordering of the folders/files in the input, but we know there is only one file.\n",
    "    for file in os.listdir(_dir):\n",
    "        input_data_path = os.path.join(\"/opt/ml/processing/input\", file)\n",
    "        if os.path.isfile(input_data_path):\n",
    "            input_data_path = os.path.join(\"/opt/ml/processing/input\", file)\n",
    "            break\n",
    "\n",
    "    assert os.path.isfile(input_data_path)\n",
    "    print(input_data_path)\n",
    "\n",
    "    try:\n",
    "        os.makedirs(\"/opt/ml/processing/train\")\n",
    "        os.makedirs(\"/opt/ml/processing/validation\")\n",
    "        os.makedirs(\"/opt/ml/processing/test\")\n",
    "        os.makedirs(\"/opt/ml/processing/train_data_with_headers\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    logger.info(\"Reading input data\")\n",
    "\n",
    "    # read csv\n",
    "    df = pd.read_csv(input_data_path)\n",
    "\n",
    "    # drop the \"Phone\" feature column\n",
    "    df = df.drop([\"Phone\"], axis=1)\n",
    "\n",
    "    # Change the data type of \"Area Code\"\n",
    "    df[\"Area Code\"] = df[\"Area Code\"].astype(object)\n",
    "\n",
    "    # Drop several other columns\n",
    "    df = df.drop([\"Day Charge\", \"Eve Charge\", \"Night Charge\", \"Intl Charge\"], axis=1)\n",
    "\n",
    "    # Convert categorical variables into dummy/indicator variables.\n",
    "    model_data = pd.get_dummies(df)\n",
    "\n",
    "    # Create one binary classification target column\n",
    "    model_data = pd.concat(\n",
    "        [\n",
    "            model_data[\"Churn?_True.\"],\n",
    "            model_data.drop([\"Churn?_False.\", \"Churn?_True.\"], axis=1),\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    model_data = model_data.rename(columns={\"Churn?_True.\": \"Churn_True\"})\n",
    "\n",
    "    # Split the data\n",
    "    train_data, validation_data, test_data = np.split(\n",
    "        model_data.sample(frac=1, random_state=1729),\n",
    "        [int(0.7 * len(model_data)), int(0.9 * len(model_data))],\n",
    "    )\n",
    "\n",
    "    train_data.to_csv(\"/opt/ml/processing/train/train.csv\", header=False, index=False)\n",
    "    train_data.to_csv(\n",
    "        \"/opt/ml/processing/train_data_with_headers/train.csv\", header=True, index=False\n",
    "    )\n",
    "    validation_data.to_csv(\n",
    "        \"/opt/ml/processing/validation/validation.csv\", header=False, index=False\n",
    "    )\n",
    "    test_data.to_csv(\"/opt/ml/processing/test/test.csv\", header=False, index=False)\n",
    "\n",
    "    train_data = train_data.drop([\"Churn_True\"], axis=1)\n",
    "    train_data.to_csv(\n",
    "        \"/opt/ml/processing/data_baseline_with_headers/baseline.csv\",\n",
    "        header=True,\n",
    "        index=False,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../algortihms/3-postprocessing/evaluate.py\n",
    "\n",
    "\"\"\"Evaluation script for measuring model accuracy.\"\"\"\n",
    "import json\n",
    "import logging\n",
    "import pathlib\n",
    "import pickle\n",
    "import tarfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    confusion_matrix,\n",
    "    roc_curve,\n",
    ")\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addHandler(logging.StreamHandler())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model_path = \"/opt/ml/processing/model/model.tar.gz\"\n",
    "    with tarfile.open(model_path) as tar:\n",
    "        tar.extractall(path=\"..\")\n",
    "\n",
    "    logger.debug(\"Loading xgboost model.\")\n",
    "    model = pickle.load(open(\"xgboost-model\", \"rb\"))\n",
    "\n",
    "    logger.debug(\"Loading test input data.\")\n",
    "    test_path = \"/opt/ml/processing/test/test.csv\"\n",
    "    df = pd.read_csv(test_path, header=None)\n",
    "    sample_payload = df.sample()\n",
    "\n",
    "    logger.debug(\"Reading test data.\")\n",
    "    y_test = df.iloc[:, 0].to_numpy()\n",
    "    df.drop(df.columns[0], axis=1, inplace=True)\n",
    "    X_test = xgboost.DMatrix(df.values)\n",
    "\n",
    "    logger.info(\"Performing predictions against test data.\")\n",
    "    prediction_probabilities = model.predict(X_test)\n",
    "    predictions = np.round(prediction_probabilities)\n",
    "\n",
    "    precision = precision_score(y_test, predictions)\n",
    "    recall = recall_score(y_test, predictions)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    conf_matrix = confusion_matrix(y_test, predictions)\n",
    "    fpr, tpr, _ = roc_curve(y_test, prediction_probabilities)\n",
    "\n",
    "    logger.debug(\"Accuracy: {}\".format(accuracy))\n",
    "    logger.debug(\"Precision: {}\".format(precision))\n",
    "    logger.debug(\"Recall: {}\".format(recall))\n",
    "    logger.debug(\"Confusion matrix: {}\".format(conf_matrix))\n",
    "\n",
    "    # Available metrics to add to model: https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-model-quality-metrics.html\n",
    "    report_dict = {\n",
    "        \"binary_classification_metrics\": {\n",
    "            \"accuracy\": {\"value\": accuracy, \"standard_deviation\": \"NaN\"},\n",
    "            \"precision\": {\"value\": precision, \"standard_deviation\": \"NaN\"},\n",
    "            \"recall\": {\"value\": recall, \"standard_deviation\": \"NaN\"},\n",
    "            \"confusion_matrix\": {\n",
    "                \"0\": {\"0\": int(conf_matrix[0][0]), \"1\": int(conf_matrix[0][1])},\n",
    "                \"1\": {\"0\": int(conf_matrix[1][0]), \"1\": int(conf_matrix[1][1])},\n",
    "            },\n",
    "            \"receiver_operating_characteristic_curve\": {\n",
    "                \"false_positive_rates\": list(fpr),\n",
    "                \"true_positive_rates\": list(tpr),\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "\n",
    "    output_dir = \"/opt/ml/processing/evaluation\"\n",
    "    sample_dir = \"/opt/ml/processing/sample\"\n",
    "\n",
    "    pathlib.Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    pathlib.Path(sample_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    sample_payload.to_csv(f\"{sample_dir}/payload.csv\", header=False, index=False)\n",
    "\n",
    "    evaluation_path = f\"{output_dir}/evaluation.json\"\n",
    "    with open(evaluation_path, \"w\") as f:\n",
    "        f.write(json.dumps(report_dict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
